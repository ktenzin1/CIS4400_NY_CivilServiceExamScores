{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Baruch_Student/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Download Libraries\n",
    "!pip3 install google-cloud-storage\n",
    "!pip3 install pyarrow # Apache Arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from io import StringIO\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import datetime\n",
    "from google.cloud import bigquery\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gathering Data\n",
    "#Extracting from datasource to view the head \n",
    "\n",
    "URL = 'https://data.cityofnewyork.us/api/views/vx8i-nprf/rows.csv?accessType=DOWNLOAD'\n",
    "\n",
    "# Define the data type for column 17 because it contains mixed data types including NaN\n",
    "dtype_mapping = {'Veteran Credit': 'object'}\n",
    "\n",
    "# Specify NaN values to be considered as missing values\n",
    "na_values = ['NaN', '', 'NA', 'nan']\n",
    "\n",
    "df_raw = pd.read_csv(URL, dtype=dtype_mapping, na_values=na_values, low_memory=False)\n",
    "\n",
    "print(df_raw.info())\n",
    "print('\\n')\n",
    "print(df_raw.shape)\n",
    "print('\\n')\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing Data\n",
    "# Specify the path to your service account key file\n",
    "service_account_key_file = '/Users/karmayangchentenzin/Downloads/service_acc_key.json'\n",
    "\n",
    "# Optionally, load other configuration settings from the JSON file\n",
    "with open(service_account_key_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Fetch data from the web (CSV file)\n",
    "url = 'https://data.cityofnewyork.us/api/views/vx8i-nprf/rows.csv?accessType=DOWNLOAD'\n",
    "\n",
    "# Read the contents of the CSV file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to fetch CSV file.\")\n",
    "    exit()\n",
    "\n",
    "# Read the CSV data into a DataFrame\n",
    "try: \n",
    "    df = pd.read_csv(io.StringIO(response.text), dtype=dtype_mapping, na_values=na_values, low_memory=False)\n",
    "    # Add timestamp column\n",
    "    df['load_date'] = datetime.datetime.now() #Date the data is stored. A new column named \"load_data\" is created.\n",
    "\n",
    "    # Replace periods with underscores in column names #This had to be done before the original data was loaded from the source to GC because when creating tables in BigQuery, an error message was shown saying that field names can not contain a period (.)\n",
    "    df.columns = df.columns.str.replace('.', '_')\n",
    "\n",
    "    # Convert DataFrame to PyArrow Table\n",
    "    table = pa.Table.from_pandas(df)\n",
    "\n",
    "    # Write PyArrow Table to Parquet format\n",
    "    parquet_file_name = 'ny_civil_service_exam.parquet'  # name for the Parquet file\n",
    "    pq.write_table(table, parquet_file_name)\n",
    "\n",
    "    # Upload Parquet file to Google Cloud Storage\n",
    "    bucket_name = 'cis4400_hw1_kyt'\n",
    "    blob_name = parquet_file_name\n",
    "\n",
    "    client = storage.Client.from_service_account_json(service_account_key_file)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(parquet_file_name)\n",
    "\n",
    "    print(f\"Parquet file '{parquet_file_name}' uploaded to {bucket_name}/{blob_name} in Google Cloud Storage.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display DataFrame with added timestamp column\n",
    "print(df.head())\n",
    "\n",
    "# Verify Parquet file contents\n",
    "parquet_table = pq.read_table(parquet_file_name)\n",
    "parquet_df = parquet_table.to_pandas()\n",
    "print(parquet_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
